# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vfg_BwuzGy6ewh4oKfQ9kOZnEbasUqKg
"""

import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

# Dataset de entrenamiento para la puerta XOR
x_train = np.array([[0,0], [0,1], [1,0], [1,1]], dtype="float32")
y_train = np.array([[0], [1], [1], [0]], dtype="float32")

# Modelo MLP (Perceptrón Multicapa)
model = keras.Sequential()
model.add(layers.Dense(2, input_dim=2, activation='relu'))   # Capa oculta
model.add(layers.Dense(1, activation='sigmoid'))             # Capa de salida

# Configuración del modelo
model.compile(
    optimizer=keras.optimizers.Adam(0.1),
    loss='mean_squared_error',
    metrics=['accuracy']
)

# Entrenamiento del modelo
fit_history = model.fit(x_train, y_train, epochs=50, batch_size=4, verbose=1)

# Graficamos la pérdida
loss_curve = fit_history.history['loss']
accuracy_curve = fit_history.history['accuracy']

plt.plot(loss_curve, label='Pérdida')
plt.plot(accuracy_curve, label='Precisión')
plt.legend(loc='lower left')
plt.title('Resultado del Entrenamiento - Puerta XOR')
plt.xlabel('Épocas')
plt.ylabel('Valor')
plt.show()

# Recuperamos pesos y sesgos de cada capa
weights_HL, biases_HL = model.layers[0].get_weights()  # Capa oculta
weights_OL, biases_OL = model.layers[1].get_weights()  # Capa de salida

print("Pesos capa oculta:\n", weights_HL)
print("Sesgos capa oculta:\n", biases_HL)
print("Pesos capa salida:\n", weights_OL)
print("Sesgos capa salida:\n", biases_OL)

# Predicción
prediccion = model.predict(x_train)
print("\nPredicciones:")
print(prediccion)

print("\nEntradas:")
print(x_train)

print("\nSalidas esperadas:")
print(y_train)

# Clasificación Supervisada con K-Nearest Neighbors (KNN)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1. Cargar dataset real (flores Iris)
iris = load_iris()
X, y = iris.data, iris.target

# 2. Separar en entrenamiento y prueba (70% entrenamiento, 30% prueba)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Entrenar modelo supervisado (KNN)
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

# 4. Evaluar el modelo
y_pred = model.predict(X_test)
print("Exactitud del modelo:", accuracy_score(y_test, y_pred))

# 5. Predicción con nuevos datos
nueva_flor = [[5.1, 3.5, 1.4, 0.2]]
prediccion = iris.target_names[model.predict(nueva_flor)][0]
print("Predicción para la flor nueva:", prediccion)

# Clustering (Aprendizaje NO Supervisado) con K-Means

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1. Crear datos artificiales
# Generamos 200 puntos distribuidos en 3 grupos (centros)
X, _ = make_blobs(n_samples=200, centers=3, random_state=42)

# 2. Entrenar modelo no supervisado (K-Means)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# 3. Visualizar resultados
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap="viridis", s=30)
plt.scatter(
    kmeans.cluster_centers_[:, 0],
    kmeans.cluster_centers_[:, 1],
    c="red",
    marker="X",
    s=200,
    label="Centroides"
)
plt.title("Clustering con K-Means")
plt.legend()
plt.show()

# Red Neuronal Simple - Backpropagation (Puerta XOR)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# 1. Datos de entrenamiento (problema XOR)
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])

# 2. Crear red neuronal
# Capa oculta con 4 neuronas y activación ReLU
# Capa de salida con 1 neurona y activación Sigmoid
model = Sequential([
    Dense(4, activation='relu', input_shape=(2,)),
    Dense(1, activation='sigmoid')
])

# 3. Compilar el modelo
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 4. Entrenar el modelo
model.fit(X, y, epochs=500, verbose=0)

# 5. Evaluar y mostrar predicciones
print("Predicciones XOR:")
print(X, "\n", model.predict(X).round())